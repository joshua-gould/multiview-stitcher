{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"multiview-stitcher","text":"<p><code>multiview-stitcher</code> is an open-source modular toolbox for distributed and tiled stitching of 2-3D image data in python. It is an extensible framework including a collection of algorithms to register and fuse small and large datasets from multi-positioning and multi-view light sheet microscopy, as well as other modalities such as correlative cryo-EM datasets.</p> <p>For visualization, the associated <code>napari-stitcher</code> provides visualization functionality using the Napari viewer, including a standalone widget.</p> <p>With a focus on interoperability and integration with existing tools and the ecosystem, the package intends to integrate as tightly as possible with the NGFF specification.</p> <p>It leverages <code>xarray</code> in combination with <code>spatial-image</code> classes for image metadata handling and <code>dask</code> (and <code>dask-image</code>) for chunked and distributed image processing.</p>"},{"location":"#napari-plugin","title":"Napari plugin","text":"<p>There's an associated napari plugin: napari-stitcher.</p>"},{"location":"#previous-work","title":"Previous work","text":"<p><code>multiview-stitcher</code> improves and replaces MVRegFus.</p> <p>Note</p> <p><code>multiview-stitcher</code> is a work in progress. The API is not yet stable.</p>"},{"location":"chart/","title":"Workflow chart","text":""},{"location":"chart/#higher-level","title":"Higher-level","text":"<pre><code>%%{init: { \"graph\": { \"htmlLabels\": true, \"curve\": \"linear\" } } }%%\ngraph TD\n%%   A[Numpy-like arrays] --&gt; |Define geometry| B{Tiles};\n  Tiles@{ shape: procs, label: \"Tiles\"}\n\n  Tile_i@{ shape: tag-rect, label: \"Tile i\" }\n  TileN@{ shape: brace-r, label: \"further tiles...\" }\n\n  ArrayNP@{ shape: win-pane, label: \"Numpy\" }\n  ArrayDA@{ shape: win-pane, label: \"Dask\" }\n  ArrayCP@{ shape: win-pane, label: \"CuPy\" }\n\n  Scale@{ shape: notch-rect, label: \"Scale\\n Translation\" }\n  Dimensions@{ shape: notch-rect, label: \"Image dimensions\\n(C,) (T,) (Z,) Y, X\" }\n\n  Fused@{ shape: tag-rect, label: \"Fused image\" }\n  Fuse@{ shape: rounded, label: \"Fuse tiles\"}\n\n  GraphConst@{ shape: rounded, label: \"Construct graph\" }\n  PairReg@{ shape: rounded, label: \"Pairwise\\n view registration\" }\n  GloRes@{ shape: rounded, label: \"Global parameter\\n resolution\" }\n  Params@{ shape: procs, label: \"Transform parameters\"}\n\n  Storage@{ shape: lin-cyl, label: \"Storage\" }\n\n  Tile_i --&gt; Tiles\n%%   Tile2 --&gt; Tiles\n  TileN --&gt; Tiles\n\n  Tiles --&gt; |Select initial transform_key| GraphConst\n  Params --&gt; |Set new transform_key| Tiles\n\n  Tiles --&gt; |Select transform_key| Fuse\n\n  subgraph Input tile definition\n    subgraph Input-arrays\n        ArrayNP\n        ArrayDA\n        ArrayCP\n    end\n    Input-arrays --&gt; Tile_i\n    Scale --&gt; |Defines default transform_key| Tile_i\n    Dimensions --&gt; Tile_i\n    %% Array2 --&gt; Tile1\n    %% Scale2 --&gt; Tile2\n    TileN\n  end\n\n  subgraph Registration\n  GraphConst --&gt; PairReg\n  PairReg --&gt; GloRes\n  GloRes --&gt; Params\n  end\n\n  subgraph Fusion\n    Fuse --&gt; Fused\n    Fused --&gt; Storage\n  end\n</code></pre>"},{"location":"chart/#lower-level","title":"Lower-level","text":"<pre><code>graph TD\n%%   A[Numpy-like arrays] --&gt; |Define geometry| B{Tiles};\n  Tiles@{ shape: procs, label: \"Tiles\"}\n\n  Tile_i@{ shape: tag-rect, label: \"Tile i\" }\n  TileN@{ shape: brace-r, label: \"further tiles...\" }\n\n  ArrayNP@{ shape: win-pane, label: \"Numpy\" }\n  ArrayDA@{ shape: win-pane, label: \"Dask\" }\n  ArrayCP@{ shape: win-pane, label: \"CuPy\" }\n\n  Scale@{ shape: notch-rect, label: \"Scale Translation\" }\n  Dimensions@{ shape: notch-rect, label: \"Image dimensions\\n(C,) (T,) (Z,) Y, X\" }\n\n  Fused@{ shape: tag-rect, label: \"Fused image\" }\n  Fuse@{ shape: circle, label: \"Fuse tiles\"}\n\n%%   Fusion-methods@{ shape: brace-r, label: \"**Fusion methods**\n%%     - weighted average\n%%     - multi-view deconvolution\n%%     - custom API\n%% \" }\n\n  Weighted_average@{ shape: rounded, label: \"Weighted average\" }\n  Max_fusion@{ shape: rounded, label: \"Maximum intensity\" }\n  Multi-view-deconvolution@{ shape: rounded, label: \"Multi-view deconvolution\" }\n  Custom-fusion@{ shape: rounded, label: \"Custom API\" }\n\n  Blending_weights@{ shape: rect, label: \"Linear blending\" }\n  Content_weights@{ shape: rect, label: \"Content-based\" }\n  Custom_weights@{ shape: rect, label: \"Custom API\" }\n\n  subgraph Graph-construction[\"View graph construction\"]\n  direction TB\n    Tile1 --&gt; |overlaps| Tile2 &amp; Tile3--&gt; |overlaps| Tile4\n  end\n\n  subgraph Pairwise-registration\n    direction RL\n    RegPhase@{ shape: rounded, label: \"Phase correlation\" }\n    RegAnts@{ shape: rounded, label: \"AntsPy\" }\n    RegElastix@{ shape: rounded, label: \"ITKElastix\" }\n    RegCustom@{ shape: rounded, label: \"Custom API\" }\n  end\n\n  subgraph Global-resolution\n    direction RL\n    GloRes_shortest@{ shape: rounded, label: \"Shortest Paths\" }\n    GloRes_globalopt@{ shape: rounded, label: \"Global Optimization\" }\n    GloRes_custom@{ shape: rounded, label: \"Custom API\" }\n  end\n\n  Params@{ shape: procs, label: \"Transform parameters\"}\n\n  Storage@{ shape: lin-cyl, label: \"Storage\" }\n\n  Tile_i --&gt; Tiles\n  TileN --&gt; Tiles\n\n  Tiles --&gt; |Select initial transform_key| Graph-construction\n  Params --&gt; |Set new transform_key| Tiles\n\n  Tiles --&gt; |Select transform_key| Fuse\n\n  subgraph Input tile definition\n    subgraph Input-arrays\n        ArrayNP\n        ArrayDA\n        ArrayCP\n    end\n    Input-arrays --&gt; Tile_i\n    Scale --&gt; |Defines default transform_key| Tile_i\n    Dimensions --&gt; Tile_i\n    TileN\n  end\n\n  subgraph Registration\n  Graph-construction --&gt; Pairwise-registration\n  Pairwise-registration --&gt; Global-resolution\n  Global-resolution --&gt; Params\n  end\n\n  subgraph Fusion\n    direction LR\n\n  subgraph Fuse\n\n    direction TB\n\n    subgraph Fusion-methods[Fusion methods]\n      direction RL\n      Weighted_average\n      Max_fusion\n      Multi-view-deconvolution\n      Custom-fusion\n    end\n\n    subgraph Weighting-methods[Weighting methods]\n      direction RL\n      Blending_weights\n      Content_weights\n      Custom_weights\n    end\n\n    end\n\n    Fuse --&gt; Fused\n    Fused --&gt; Storage\n  end\n</code></pre>"},{"location":"code_example/","title":"Code example","text":"<p>These code snippets walk you through a small stitching workflow consisting of</p> <ol> <li>Preparing the input image data and metadata (tile positions, spacing, channels)</li> <li>Registering the tiles</li> <li>Stitching / fusing the tiles</li> </ol>"},{"location":"code_example/#1-prepare-data-for-stitching","title":"1) Prepare data for stitching","text":"<pre><code>import numpy as np\nfrom multiview_stitcher import msi_utils\nfrom multiview_stitcher import spatial_image_utils as si_utils\n\n# input data (can be any numpy compatible array: numpy, dask, cupy, etc.)\ntile_arrays = [np.random.randint(0, 100, (2, 10, 100, 100)) for _ in range(3)]\n\n# indicate the tile offsets and spacing\ntile_translations = [\n    {\"z\": 2.5, \"y\": -10, \"x\": 30},\n    {\"z\": 2.5, \"y\": 30, \"x\": 10},\n    {\"z\": 2.5, \"y\": 30, \"x\": 50},\n]\nspacing = {\"z\": 2, \"y\": 0.5, \"x\": 0.5}\n\nchannels = [\"DAPI\", \"GFP\"]\n\n# build input for stitching\nmsims = []\nfor tile_array, tile_translation in zip(tile_arrays, tile_translations):\n    sim = si_utils.get_sim_from_array(\n        tile_array,\n        dims=[\"c\", \"z\", \"y\", \"x\"],\n        scale=spacing,\n        translation=tile_translation,\n        transform_key=\"stage_metadata\",\n        c_coords=channels,\n    )\n    msims.append(msi_utils.get_msim_from_sim(sim, scale_factors=[]))\n\n# plot the tile configuration\n# from multiview_stitcher import vis_utils\n# fig, ax = vis_utils.plot_positions(msims, transform_key='stage_metadata', use_positional_colors=False)\n</code></pre>"},{"location":"code_example/#2-register-the-tiles","title":"2) Register the tiles","text":"<pre><code>from dask.diagnostics import ProgressBar\nfrom multiview_stitcher import registration\n\nwith ProgressBar():\n    params = registration.register(\n        msims,\n        reg_channel=\"DAPI\",  # channel to use for registration\n        transform_key=\"stage_metadata\",\n        new_transform_key=\"translation_registered\",\n    )\n\n# plot the tile configuration after registration\n# vis_utils.plot_positions(msims, transform_key='translation_registered', use_positional_colors=False)\n</code></pre>"},{"location":"code_example/#3-stitch-fuse-the-tiles","title":"3) Stitch / fuse the tiles","text":"<pre><code>from multiview_stitcher import fusion\n\nfused_sim = fusion.fuse(\n    [msi_utils.get_sim_from_msim(msim) for msim in msims],\n    transform_key=\"translation_registered\",\n)\n\n# get fused array as a dask array\nfused_sim.data\n\n# get fused array as a numpy array\nfused_sim.data.compute()\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are very welcome.</p>"},{"location":"data_formats/","title":"Data formats","text":"<p>Note</p> <p><code>multiview-stitcher</code> works with any numpy-like input arrays. Therefore, as long as the data can be read into a numpy array, it can be used with <code>multiview-stitcher</code>.</p> <p>For attaching metadata to arrays, multiview-stitcher works with SpatialImage objects (with additional transform matrices attached). They can be constructed from Numpy, Dask or CuPy arrays as such:</p> <pre><code>from multiview_stitcher import spatial_image_utils as si_utils\nsim = si_utils.get_sim_from_array(\n    tile_array,\n    dims=[\"c\", \"y\", \"x\"],\n    scale={'y': 0.5, 'x': 0.5},\n    translation={\"y\": 30, \"x\": 50},\n    transform_key=\"stage_metadata\",\n    c_coords=['DAPI', 'GFP'],\n)\n</code></pre> <p>A multiscale version of this object is represented by instances of MultiscaleSpatialImage, which can be created as such:</p> <pre><code>from multiview_stitcher import msi_utils\nmsim = msi_utils.get_msim_from_sim(sim, scale_factors=[2, 4])\n</code></pre> <p>The following code can be used to extract a given scale from a multiscale image:</p> <pre><code>sim = msi_utils.get_sim_from_msim(msim, scale=\"scale0\")\n</code></pre>"},{"location":"data_formats/#ome-zarr","title":"OME-Zarr","text":"<p>Note</p> <p>NGFF 0.4 (the latest OME-Zarr standard) currently only supports translation transformations. Therefore, affine transformations cannot yet be stored in OME-Zarr files.</p> <p>Some support for reading and writing OME-Zarrs is provided by multiscaleimage.MultiscaleImage.</p> <p>Further, <code>multiview_stitcher.ngff_utils</code> provides some convenience functions for reading and writing OME-Zarrs using <code>ngff-zarr</code>.</p>"},{"location":"data_formats/#further-file-formats","title":"Further file formats","text":"<p><code>bioio</code> is a very convenient library for reading a large variety of image files and it includes support for lazy loading. Here's example code of how to use <code>bioio</code> to load an image file into a tile compatible with <code>multiview-stitcher</code>:</p> <pre><code>import bioio\nfrom multiview_stitcher import spatial_image_utils as si_utils\n\n# use bioio to load the image as a xarray.DataArray\nbioio_xr = BioImage(\"my_file.tiff\").get_xarray_dask_stack().squeeze()\n\nsim = si_utils.get_sim_from_array(\n    bioio_xr.data,\n    dims=bioio_xr.dims,\n    scale=si_utils.get_spatial_dims_from_sim(bioio_xr),\n    translation=si_utils.get_origin_from_sim(bioio_xr),\n    c_coords=bioio_xr.coords[\"c\"].values,\n    transform_key=\"stage_metadata\",\n)\n</code></pre>"},{"location":"features/","title":"Features","text":"<p>Below is a list of features which are either already implemented or are on the roadmap.</p>"},{"location":"features/#dimensionality","title":"Dimensionality","text":"<ul> <li> 2D</li> <li> 3D</li> </ul>"},{"location":"features/#registration","title":"Registration","text":""},{"location":"features/#pairwise-registration-methods","title":"Pairwise registration methods","text":"<ul> <li> Phase correlation</li> <li> ANTsPy</li> <li> Elastix (<code>itk-elastix</code>)</li> <li> Bead alignment</li> <li> Phase correlation for rotation + translation</li> </ul>"},{"location":"features/#global-paramater-resolution","title":"Global paramater resolution","text":"<ul> <li> Graph construction</li> <li> Automatic determination of reference view</li> <li> Parameter concatenation along graph connectivity paths</li> <li> Global optimization of registration parameters from (potentially overdetermined) pairwise transforms</li> </ul>"},{"location":"features/#transformations","title":"Transformations","text":"<ul> <li> Chunked <code>dask_image.ndinterp.affine_transform</code></li> <li> Cupy-based transform</li> <li> Chaining transformations instead of working with static coordinate systems</li> </ul>"},{"location":"features/#fusion","title":"Fusion","text":""},{"location":"features/#general","title":"General","text":"<ul> <li> Modular API to plug in different fusion and weight functions</li> <li> Support for fusion label maps</li> <li> Cupy-based fusion</li> </ul>"},{"location":"features/#supported-fusion-methods","title":"Supported fusion methods:","text":"<ul> <li> Weighted average</li> <li> Maximum intensity projection</li> <li> Multi-view deconvolution</li> </ul>"},{"location":"features/#supported-weights","title":"Supported weights:","text":"<ul> <li> Linear blending</li> <li> Content-based</li> </ul>"},{"location":"features/#supported-data-formats","title":"Supported data formats","text":"<ul> <li> OME-Zarr</li> <li> Zarr based intermediate file format for reading and writing, compatible with parallel dask workflows: multiscale-spatial-data</li> <li>CZI input</li> <li> Multi-positioning</li> <li> Light-sheet</li> <li> TIF input</li> <li> TIF writing</li> </ul>"},{"location":"features/#visualization","title":"Visualization","text":""},{"location":"features/#napari","title":"Napari","text":"<p>See napari-stitcher.</p> <ul> <li> 2D slice view: multiscale rendering</li> <li>3D rendered view:</li> <li> Lowest scale</li> <li> Chunked rendering</li> <li> Colormaps optimized for highlighting differences between overlapping views</li> </ul>"},{"location":"features/#supported-usage-modes","title":"Supported usage modes","text":"<ul> <li> As a library to build custom reconstruction workflows</li> <li> Napari plugin</li> <li> Convenience function for processing on HPC</li> </ul>"},{"location":"implementation_details/","title":"Implementation details","text":""},{"location":"implementation_details/#image-data-structures","title":"(Image) data structures","text":""},{"location":"implementation_details/#affine-transformations","title":"Affine transformations","text":"<p>Affine transformations associated to an image / view are represented as <code>xarray.DataArray</code>s with dimensions (t, x_in, x_out), typically of shape (N_tps, ndim+1, ndim+1). There's one transform per timepoint.</p>"},{"location":"implementation_details/#in-memory-representation","title":"In memory representation","text":""},{"location":"implementation_details/#spatial-image","title":"spatial-image","text":"<p>Subclasses <code>xarray.DataArray</code>, i.e. broadly speaking these are numpy/dask arrays with axis labels, coordinates and attributes. spatial-image is dask compatible for lazy loading and parallel processing.</p>"},{"location":"implementation_details/#multiscale-spatial-image","title":"multiscale-spatial-image","text":"<ul> <li><code>xarray.datatree</code> containing one <code>xarray.Dataset</code> per (hierarchical) spatial scale</li> <li>these are collections of <code>xarray.DataArray</code> which are called \"data variables\" and share coordinates.</li> <li>each scale contains a <code>spatial-image</code>s as a data variable named 'image'</li> <li>compatible with NGFF (github.com/ome/ngff)</li> <li>can be (de-)serialized to zarr</li> <li>also used by github.com/scverse/spatialdata</li> </ul>"},{"location":"implementation_details/#coordinate-systems","title":"Coordinate systems","text":"<p>spatial-image, multiscale-spatial-image, as well as NGFF (as of 0.4.1), do not yet support:</p> <ul> <li>affine transformations</li> <li>different transformations for different timepoints.</li> </ul> <p>However, affine transformations are important for positioning views relatively to each other. Therefore, <code>spatial-image</code> and <code>multiscale-spatial-image</code> are used with modifications. Specifically, affine transformation parameters which transform the image into a coordinate system of a given name are attached to both: - <code>spatial-image</code>: as key(name)/value pairs under a 'transform' attribute - <code>multiscale-spatial-image</code>: to each scale as data variables, sharing the 't' coordinate with the associated image data variable. This is compatible with reading and writing <code>multiscale_spatial_image.to_zarr()</code> and <code>datatree.open_zarr()</code>.</p> <p>In the code, coordinate systems are referred to as transform_key (TODO: find better name, e.g. coordinate_system).</p>"},{"location":"implementation_details/#registration","title":"Registration","text":""},{"location":"implementation_details/#overlap-graph","title":"Overlap graph","text":"<p>An overlap graph is computed from the input images (represented as a directed <code>networkx.DiGraph</code>) in which the - nodes represent the views and - edges represent geometrical overlap.</p> <p>This graph can be used to conveniently color views for visualization (overlapping views should have different colors, but the total number of colors used shouldn't be too large, i.e. exceed 2-4).</p>"},{"location":"implementation_details/#reference-view","title":"Reference view","text":"<p>A suitable reference view can be obtained from the overlap graph by e.g. choosing the view with maximal overlap to other views.</p>"},{"location":"implementation_details/#registration-graph","title":"Registration graph","text":"<p>A registration graph or list of registration pairs (TODO: clarify whether this should be a graph or a list of pairs) is obtained from the overlap graph by e.g. finding shortest overlap-weighted paths between the reference view and all other views.</p>"},{"location":"implementation_details/#fusion","title":"Fusion","text":""},{"location":"implementation_details/#fusion-framework","title":"Fusion framework","text":"<p><code>multiview_stitcher.fusion.fuse(..., fusion_func=fusion.weighted_average_fusion)</code> can be used in combination with fusion functions available in <code>multiview_stitcher.fusion</code> or custom functions that accept the following keyword arguments:</p> <pre><code>transformed_views : list of ndarrays\n    transformed input views\nblending_weights : list of ndarrays, optional\n    blending weights for each view\nfusion_weights : list of ndarrays, optional\n    additional view weights for fusion, e.g. contrast weighted scores.\nparams : list of xarrays, optional\n</code></pre> <p>Further fusion weights can be obtained by the weight calculation function specified in <code>fuse(..., weights_func=None, weights_func_kwargs=None)</code>. An example for such a function is <code>weights.content_based</code>, but also custom functions can be passed which accept the same (optional) input arguments as the fusion functions.</p>"},{"location":"implementation_details/#content-based-fusion","title":"Content based fusion","text":"<p>To improve multi-view fusion in the context of strongly scattering samples, content-based fusion turns out to be helpful. This fusion method is available in <code>multiview-stitcher</code> by using <code>multiview_stitcher.fusion.fuse(..., fusion_func=fusion.weighted_average_fusion, weights_method=weights.content_based)</code>.</p>"},{"location":"installation/","title":"Installation","text":"<p>You can install <code>multiview-stitcher</code> via pip:</p> <pre><code>pip install multiview-stitcher\n</code></pre> <p>To install latest development version:</p> <pre><code>pip install git+https://github.com/multiview-stitcher/multiview-stitcher.git\n</code></pre>"},{"location":"napari_stitcher/","title":"Napari plugin","text":"<p><code>multiview-stitcher</code> has an associated napari plugin, napari-stitcher.</p> <p>A space we're watching closely is the advancement of napari towards multi-scale 3D rendering and improved 3D interactivity.</p> <p></p>"},{"location":"notebooks/","title":"Notebooks","text":"<p>Example notebooks can be found here: multiview-stitcher/notebooks.</p>"},{"location":"objects/","title":"Objects","text":""},{"location":"objects/#image","title":"Image","text":"<p>Modified instances of <code>multiscaleimage.MultiscaleImage</code>.</p> <p>Modification: Each scale has at least one named affine transform parameter attached to it as further data variables next to <code>scale&lt;scale&gt;/image</code>.</p> <p>While instances of <code>multiscale-spatial-image</code> can be serialized to and from NGFF, modified instances of <code>multiscaleimage.MultiscaleImage</code> as used by <code>multiview-stitcher</code> cannot (yet) be serialized to and from NGFF (see here), as the support for affine transforms is missing.</p> <p>Example string representation:</p> <p><code>print(msims[0])</code> <pre><code>DataTree('None', parent=None)\n\u2502   Dimensions:  ()\n\u2502   Data variables:\n\u2502       *empty*\n\u2502   Attributes:\n\u2502       multiscaleSpatialImageVersion:  1\n\u2502       multiscales:                    [{'@type': 'ngff:Image', 'axes': [{'name'...\n\u251c\u2500\u2500 DataTree('scale0')\n\u2502       Dimensions:          (t: 1, x_in: 4, x_out: 4, c: 1, z: 179, y: 1040, x: 1392)\n\u2502       Coordinates:\n\u2502         * c                (c) int64 0\n\u2502         * t                (t) int64 0\n\u2502         * x                (x) float64 0.0 0.645 1.29 1.935 ... 895.9 896.6 897.2\n\u2502         * y                (y) float64 0.0 0.645 1.29 1.935 ... 668.9 669.5 670.2\n\u2502         * z                (z) float64 0.0 2.58 5.16 7.74 ... 451.5 454.1 456.7 459.2\n\u2502       Dimensions without coordinates: x_in, x_out\n\u2502       Data variables:\n\u2502           affine_metadata  (t, x_in, x_out) float64 1.0 0.0 0.0 0.0 ... 0.0 0.0 1.0\n\u2502           image            (t, c, z, y, x) uint16 dask.array&lt;chunksize=(1, 1, 179, 256, 256), meta=np.ndarray&gt;\n\u251c\u2500\u2500 DataTree('scale1')\n\u2502       Dimensions:          (t: 1, x_in: 4, x_out: 4, c: 1, z: 179, y: 520, x: 696)\n\u2502       Coordinates:\n\u2502         * c                (c) int64 0\n\u2502         * t                (t) int64 0\n\u2502         * x                (x) float64 0.3225 1.613 2.902 4.193 ... 894.3 895.6 896.9\n\u2502         * y                (y) float64 0.3225 1.613 2.902 4.193 ... 667.3 668.5 669.8\n\u2502         * z                (z) float64 0.0 2.58 5.16 7.74 ... 451.5 454.1 456.7 459.2\n\u2502       Dimensions without coordinates: x_in, x_out\n\u2502       Data variables:\n\u2502           affine_metadata  (t, x_in, x_out) float64 1.0 0.0 0.0 0.0 ... 0.0 0.0 1.0\n\u2502           image            (t, c, z, y, x) uint16 dask.array&lt;chunksize=(1, 1, 179, 256, 256), meta=np.ndarray&gt;\n\u2514\u2500\u2500 DataTree('scale2')\n        Dimensions:          (t: 1, x_in: 4, x_out: 4, c: 1, z: 179, y: 260, x: 348)\n        Coordinates:\n          * c                (c) int64 0\n          * t                (t) int64 0\n          * x                (x) float64 0.9675 3.548 6.128 8.707 ... 891.1 893.6 896.2\n          * y                (y) float64 0.9675 3.548 6.128 8.707 ... 664.0 666.6 669.2\n          * z                (z) float64 0.0 2.58 5.16 7.74 ... 451.5 454.1 456.7 459.2\n        Dimensions without coordinates: x_in, x_out\n        Data variables:\n            affine_metadata  (t, x_in, x_out) float64 1.0 0.0 0.0 0.0 ... 0.0 0.0 1.0\n            image            (t, c, z, y, x) uint16 dask.array&lt;chunksize=(1, 1, 179, 256, 256), meta=np.ndarray&gt;\n</code></pre></p>"},{"location":"objects/#transformation-parameters","title":"Transformation parameters","text":""},{"location":"objects/#affine-transformation-parameters","title":"Affine transformation parameters","text":"<p><code>xarray.DataArray</code> containing parameters in the form of - a homogeneous transform matrix - of dimensionality (ndim+1, ndim+1) - datatype <code>float</code></p> <p>with axis labels - 't' - 'x_in' - 'x_out'</p> <p>Example string representation:</p> <p><code>print(msims[0]['scale0/affine_manual'])</code> <pre><code>&lt;xarray.DataArray 'affine_manual' (t: 1, x_in: 4, x_out: 4)&gt;\narray([[[1., 0., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.]]])\nCoordinates:\n  * t        (t) int64 0\nDimensions without coordinates: x_in, x_out\n</code></pre></p>"},{"location":"related_projects/","title":"Related projects","text":""},{"location":"related_projects/#related-software","title":"Related software","text":"<ul> <li>BigStitcher</li> <li>ashlar</li> <li>TeraStitcher</li> <li>m2stitch</li> </ul>"},{"location":"related_projects/#other-cool-spaces","title":"Other cool spaces","text":"<ul> <li>ndpyramid</li> <li>affinder</li> <li>bigstream</li> <li>SpatialData</li> </ul>"},{"location":"stitching_in_the_browser/","title":"Stitching in the browser","text":"<p><code>multiview-stitcher</code> can run without installation in your browser.</p>"},{"location":"stitching_in_the_browser/#try-it-out","title":"Try it out","text":"<ul> <li>open JupyterLite in a private browser window</li> <li>upload this notebook into the jupyter lab window: notebooks/stitching_in_the_browser.ipynb</li> <li>upload files to stitch into a 'data' folder in the jupyter lab window</li> <li>follow the notebook</li> </ul>"},{"location":"stitching_in_the_browser/#limitations","title":"Limitations","text":"<ul> <li>stitching will run with a single thread</li> <li>while the code runs locally, your local file system is not directly accessible from within the browser environment</li> </ul>"},{"location":"stitching_in_the_browser/#this-cool-functionality-is-possible-thanks-to","title":"This cool functionality is possible thanks to","text":"<ul> <li>JupyterLite, a JupyterLab distribution that runs in the browser</li> <li>pyodide, a Python runtime for the browser</li> </ul>"},{"location":"api/fusion/","title":"Fusion","text":""},{"location":"api/fusion/#multiview_stitcher.fusion.fuse","title":"<code>fuse(sims, transform_key=None, fusion_func=weighted_average_fusion, weights_func=None, weights_func_kwargs=None, output_spacing=None, output_stack_mode='union', output_origin=None, output_shape=None, output_stack_properties=None, output_chunksize=None, overlap_in_pixels=None, interpolation_order=1, blending_widths=None)</code>","text":"<p>Fuse input views.</p> <p>This function fuses all (Z)YX views (\"fields\") contained in the input list of images, which can additionally contain C and T dimensions.</p>"},{"location":"api/fusion/#multiview_stitcher.fusion.fuse--parameters","title":"Parameters","text":"<p>sims : list of SpatialImage     Input views. transform_key : str, optional     Which (extrinsic coordinate system) to use as transformation parameters.     By default None (intrinsic coordinate system). fusion_func : func, optional     Fusion function to be applied. This function receives the following     inputs (as arrays if applicable): transformed_views, blending_weights, fusion_weights, params.     By default weighted_average_fusion weights_func : func, optional     Function to calculate fusion weights. This function receives the     following inputs: transformed_views (as spatial images), params.     It returns (non-normalized) fusion weights for each view.     By default None. output_spacing : dict, optional     Spacing of the fused image for each spatial dimension, by default None output_stack_mode : str, optional     Mode to determine output stack properties. Can be one of     \"union\", \"intersection\", \"sample\". By default \"union\" output_origin : dict, optional     Origin of the fused image for each spatial dimension, by default None output_shape : dict, optional     Shape of the fused image for each spatial dimension, by default None output_stack_properties : dict, optional     Dictionary describing the output stack with keys     'spacing', 'origin', 'shape'. Other output_* are ignored     if this argument is present. output_chunksize : int or dict, optional     Chunksize of the dask data array of the fused image, by default 512</p>"},{"location":"api/fusion/#multiview_stitcher.fusion.fuse--returns","title":"Returns","text":"<p>SpatialImage     Fused image.</p> Source code in <code>src/multiview_stitcher/fusion.py</code> <pre><code>def fuse(\n    sims: list,\n    transform_key: str = None,\n    fusion_func=weighted_average_fusion,\n    weights_func=None,\n    weights_func_kwargs=None,\n    output_spacing: dict[str, float] = None,\n    output_stack_mode: str = \"union\",\n    output_origin: dict[str, float] = None,\n    output_shape: dict[str, int] = None,\n    output_stack_properties: BoundingBox = None,\n    output_chunksize: Union[int, dict[str, int]] = None,\n    overlap_in_pixels: int = None,\n    interpolation_order: int = 1,\n    blending_widths: dict[str, float] = None,\n):\n    \"\"\"\n\n    Fuse input views.\n\n    This function fuses all (Z)YX views (\"fields\") contained in the\n    input list of images, which can additionally contain C and T dimensions.\n\n    Parameters\n    ----------\n    sims : list of SpatialImage\n        Input views.\n    transform_key : str, optional\n        Which (extrinsic coordinate system) to use as transformation parameters.\n        By default None (intrinsic coordinate system).\n    fusion_func : func, optional\n        Fusion function to be applied. This function receives the following\n        inputs (as arrays if applicable): transformed_views, blending_weights, fusion_weights, params.\n        By default weighted_average_fusion\n    weights_func : func, optional\n        Function to calculate fusion weights. This function receives the\n        following inputs: transformed_views (as spatial images), params.\n        It returns (non-normalized) fusion weights for each view.\n        By default None.\n    output_spacing : dict, optional\n        Spacing of the fused image for each spatial dimension, by default None\n    output_stack_mode : str, optional\n        Mode to determine output stack properties. Can be one of\n        \"union\", \"intersection\", \"sample\". By default \"union\"\n    output_origin : dict, optional\n        Origin of the fused image for each spatial dimension, by default None\n    output_shape : dict, optional\n        Shape of the fused image for each spatial dimension, by default None\n    output_stack_properties : dict, optional\n        Dictionary describing the output stack with keys\n        'spacing', 'origin', 'shape'. Other output_* are ignored\n        if this argument is present.\n    output_chunksize : int or dict, optional\n        Chunksize of the dask data array of the fused image, by default 512\n\n    Returns\n    -------\n    SpatialImage\n        Fused image.\n    \"\"\"\n\n    ndim = si_utils.get_ndim_from_sim(sims[0])\n    sdims = si_utils.get_spatial_dims_from_sim(sims[0])\n    nsdims = [dim for dim in sims[0].dims if dim not in sdims]\n\n    if output_chunksize is None:\n        output_chunksize = si_utils.get_default_spatial_chunksizes(ndim)\n\n    params = [\n        si_utils.get_affine_from_sim(sim, transform_key=transform_key)\n        for sim in sims\n    ]\n\n    if output_chunksize is None:\n        output_chunksize = si_utils.get_default_spatial_chunksizes(ndim)\n        # output_chunksize = tuple([default_chunksizes[dim] for dim in sdims])\n    elif isinstance(output_chunksize, int):\n        output_chunksize = {dim: output_chunksize for dim in sdims}\n\n    if output_stack_properties is None:\n        if output_spacing is None:\n            output_spacing = si_utils.get_spacing_from_sim(sims[0])\n\n        output_stack_properties = calc_fusion_stack_properties(\n            sims,\n            params=params,\n            spacing=output_spacing,\n            mode=output_stack_mode,\n        )\n\n        if output_origin is not None:\n            output_stack_properties[\"origin\"] = output_origin\n\n        if output_shape is not None:\n            output_stack_properties[\"shape\"] = output_shape\n\n    # determine overlap from weights method\n    # (soon: fusion methods will also require overlap)\n    overlap_in_pixels = 0\n    if weights_func is not None:\n        overlap_in_pixels = np.max(\n            [\n                overlap_in_pixels,\n                weights.calculate_required_overlap(\n                    weights_func, weights_func_kwargs\n                ),\n            ]\n        )\n\n    # calculate output chunk bounding boxes\n    output_chunk_bbs, block_indices = mv_graph.get_chunk_bbs(\n        output_stack_properties, output_chunksize\n    )\n\n    # add overlap to output chunk bounding boxes\n    output_chunk_bbs_with_overlap = [\n        output_chunk_bb\n        | {\n            \"origin\": {\n                dim: output_chunk_bb[\"origin\"][dim]\n                - overlap_in_pixels * output_stack_properties[\"spacing\"][dim]\n                for dim in sdims\n            }\n        }\n        | {\n            \"shape\": {\n                dim: output_chunk_bb[\"shape\"][dim] + 2 * overlap_in_pixels\n                for dim in sdims\n            }\n        }\n        for output_chunk_bb in output_chunk_bbs\n    ]\n\n    views_bb = [si_utils.get_stack_properties_from_sim(sim) for sim in sims]\n\n    merges = []\n    for ns_coords in itertools.product(\n        *tuple([sims[0].coords[nsdim] for nsdim in nsdims])\n    ):\n        sim_coord_dict = {\n            ndsim: ns_coords[i] for i, ndsim in enumerate(nsdims)\n        }\n        params_coord_dict = {\n            ndsim: ns_coords[i]\n            for i, ndsim in enumerate(nsdims)\n            if ndsim in params[0].dims\n        }\n\n        # ssims = [sim.sel(sim_coord_dict) for sim in sims]\n        sparams = [param.sel(params_coord_dict) for param in params]\n\n        # should this be done within the loop over output chunks?\n        fix_dims = []\n        for dim in sdims:\n            other_dims = [odim for odim in sdims if odim != dim]\n            if (\n                any((param.sel(x_in=dim, x_out=dim) - 1) for param in sparams)\n                or any(\n                    any(param.sel(x_in=dim, x_out=other_dims))\n                    for param in sparams\n                )\n                or any(\n                    any(param.sel(x_in=other_dims, x_out=dim))\n                    for param in sparams\n                )\n                or any(\n                    output_stack_properties[\"spacing\"][dim]\n                    - views_bb[iview][\"spacing\"][dim]\n                    for iview in range(len(sims))\n                )\n                or any(\n                    float(\n                        output_stack_properties[\"origin\"][dim]\n                        - param.sel(x_in=dim, x_out=\"1\")\n                    )\n                    % output_stack_properties[\"spacing\"][dim]\n                    for param in sparams\n                )\n            ):\n                continue\n            fix_dims.append(dim)\n\n        fused_output_chunks = np.empty(\n            np.max(block_indices, 0) + 1, dtype=object\n        )\n        for output_chunk_bb, output_chunk_bb_with_overlap, block_index in tqdm(\n            zip(\n                output_chunk_bbs, output_chunk_bbs_with_overlap, block_indices\n            ),\n            total=len(output_chunk_bbs),\n            desc=\"Constructing output dask array graph\",\n        ):\n            # calculate relevant slices for each output chunk\n            # this is specific to each non spatial coordinate\n            views_overlap_bb = [\n                mv_graph.get_overlap_for_bbs(\n                    target_bb=output_chunk_bb_with_overlap,\n                    query_bbs=[view_bb],\n                    param=sparams[iview],\n                    additional_extent_in_pixels={\n                        dim: 0 if dim in fix_dims else int(interpolation_order)\n                        for dim in sdims\n                    },\n                )[0]\n                for iview, view_bb in enumerate(views_bb)\n            ]\n\n            # append to output\n            relevant_view_indices = np.where(\n                [\n                    view_overlap_bb is not None\n                    for view_overlap_bb in views_overlap_bb\n                ]\n            )[0]\n\n            if not len(relevant_view_indices):\n                fused_output_chunks[tuple(block_index)] = da.zeros(\n                    tuple([output_chunk_bb[\"shape\"][dim] for dim in sdims]),\n                    dtype=sims[0].dtype,\n                )\n                continue\n\n            tol = 1e-6\n            sims_slices = [\n                sims[iview].sel(\n                    sim_coord_dict\n                    | {\n                        dim: slice(\n                            views_overlap_bb[iview][\"origin\"][dim] - tol,\n                            views_overlap_bb[iview][\"origin\"][dim]\n                            + (views_overlap_bb[iview][\"shape\"][dim] - 1)\n                            * views_overlap_bb[iview][\"spacing\"][dim]\n                            + tol,\n                        )\n                        for dim in sdims\n                    },\n                    drop=True,\n                )\n                for iview in relevant_view_indices\n            ]\n\n            # determine whether to fuse plany by plane\n            #  to avoid weighting edge artifacts\n            # fuse planewise if:\n            # - z dimension is present\n            # - params don't affect z dimension\n            # - shape in z dimension is 1 (i.e. only one plane)\n            # (the last criterium above could be dropped if we find a way\n            # (to propagate metadata through xr.apply_ufunc)\n\n            if (\n                \"z\" in fix_dims\n                and output_chunk_bb_with_overlap[\"shape\"][\"z\"] == 1\n            ):\n                fuse_planewise = True\n\n                sims_slices = [sim.isel(z=0) for sim in sims_slices]\n                tmp_params = [\n                    sparams[iview].sel(\n                        x_in=[\"y\", \"x\", \"1\"],\n                        x_out=[\"y\", \"x\", \"1\"],\n                    )\n                    for iview in relevant_view_indices\n                ]\n\n                output_chunk_bb_with_overlap = mv_graph.project_bb_along_dim(\n                    output_chunk_bb_with_overlap, dim=\"z\"\n                )\n\n                full_view_bbs = [\n                    mv_graph.project_bb_along_dim(views_bb[iview], dim=\"z\")\n                    for iview in relevant_view_indices\n                ]\n\n            else:\n                fuse_planewise = False\n                tmp_params = [\n                    sparams[iview] for iview in relevant_view_indices\n                ]\n                full_view_bbs = [\n                    views_bb[iview] for iview in relevant_view_indices\n                ]\n\n            fused_output_chunk = delayed(\n                lambda append_leading_axis, **kwargs: fuse_np(**kwargs)[\n                    np.newaxis\n                ]\n                if append_leading_axis\n                else fuse_np(**kwargs),\n            )(\n                append_leading_axis=fuse_planewise,\n                sims=sims_slices,\n                params=tmp_params,\n                output_properties=output_chunk_bb_with_overlap,\n                fusion_func=fusion_func,\n                weights_func=weights_func,\n                weights_func_kwargs=weights_func_kwargs,\n                trim_overlap_in_pixels=overlap_in_pixels,\n                interpolation_order=1,\n                full_view_bbs=full_view_bbs,\n                blending_widths=blending_widths,\n            )\n\n            fused_output_chunk = da.from_delayed(\n                fused_output_chunk,\n                shape=tuple([output_chunk_bb[\"shape\"][dim] for dim in sdims]),\n                dtype=sims[0].dtype,\n            )\n\n            fused_output_chunks[tuple(block_index)] = fused_output_chunk\n\n        fused = da.block(fused_output_chunks.tolist())\n\n        merge = si.to_spatial_image(\n            fused,\n            dims=sdims,\n            scale=output_stack_properties[\"spacing\"],\n            translation=output_stack_properties[\"origin\"],\n        )\n\n        merge = merge.expand_dims(nsdims)\n        merge = merge.assign_coords(\n            {ns_coord.name: [ns_coord.values] for ns_coord in ns_coords}\n        )\n        merges.append(merge)\n\n    if len(merges) &gt; 1:\n        # suppress pandas future warning occuring within xarray.concat\n        with warnings.catch_warnings():\n            warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n\n            # if sims are named, combine_by_coord returns a dataset\n            res = xr.combine_by_coords([m.rename(None) for m in merges])\n    else:\n        res = merge\n\n    res = si_utils.get_sim_from_xim(res)\n    si_utils.set_sim_affine(\n        res,\n        param_utils.identity_transform(len(sdims)),\n        transform_key,\n    )\n\n    # order channels in the same way as first input sim\n    # (combine_by_coords may change coordinate order)\n    if \"c\" in res.dims:\n        res = res.sel({\"c\": sims[0].coords[\"c\"].values})\n\n    return res\n</code></pre>"},{"location":"api/registration/","title":"Registration","text":""},{"location":"api/registration/#multiview_stitcher.registration.register","title":"<code>register(msims, transform_key, reg_channel_index=None, reg_channel=None, new_transform_key=None, registration_binning=None, overlap_tolerance=0.0, pairwise_reg_func=phase_correlation_registration, pairwise_reg_func_kwargs=None, groupwise_resolution_method='global_optimization', groupwise_resolution_kwargs=None, pre_registration_pruning_method='alternating_pattern', post_registration_do_quality_filter=False, post_registration_quality_threshold=0.2, plot_summary=False, pairs=None, scheduler=None)</code>","text":"<p>Register a list of views to a common extrinsic coordinate system.</p> <p>This function is the main entry point for registration.</p> <p>1) Build a graph of pairwise overlaps between views 2) Determine registration pairs from this graph 3) Register each pair of views.    Need to add option to pass registration functions here. 4) Determine the parameters mapping each view into the new extrinsic    coordinate system.    Currently done by determining a reference view and concatenating for reach    view the pairwise transforms along the shortest paths towards the ref view.</p>"},{"location":"api/registration/#multiview_stitcher.registration.register--parameters","title":"Parameters","text":"<p>msims : list of MultiscaleSpatialImage     Input views reg_channel_index : int, optional     Index of channel to be used for registration, by default None reg_channel : str, optional     Name of channel to be used for registration, by default None     Overrides reg_channel_index transform_key : str, optional     Extrinsic coordinate system to use as a starting point     for the registration, by default None new_transform_key : str, optional     If set, the registration result will be registered as a new extrinsic     coordinate system in the input views (with the given name), by default None registration_binning : dict, optional     Binning applied to each dimensionn during registration, by default None overlap_tolerance : float, optional     Extend overlap regions considered for pairwise registration.     - if 0, the overlap region is the intersection of the bounding boxes.     - if &gt; 0, the overlap region is the intersection of the bounding boxes         extended by this value in all spatial dimensions.     - if None, the full images are used for registration pairwise_reg_func : func, optional     Function used for registration. pairwise_reg_func_kwargs : dict, optional     Additional keyword arguments passed to the registration function groupwise_resolution_method : str, optional     Method used to determine the final transform parameters     from pairwise registrations:     - 'global_optimization': global optimization considering all pairwise transforms     - 'shortest_paths': concatenation of pairwise transforms along shortest paths groupwise_resolution_kwargs : dict, optional     Additional keyword arguments passed to the groupwise optimization function pre_registration_pruning_method : str, optional     Method used to eliminate registration edges (e.g. diagonals) from the view adjacency     graph before registration. Available methods:     - None: No pruning, useful when no regular arrangement is present.     - 'alternating_pattern': Prune to edges between squares of differering         colors in checkerboard pattern. Useful for regular 2D tile arrangements (of both 2D or 3D data).     - 'shortest_paths_overlap_weighted': Prune to shortest paths in overlap graph         (weighted by overlap). Useful to minimize the number of pairwise registrations.     - 'otsu_threshold_on_overlap': Prune to edges with overlap above Otsu threshold.         This is useful for regular 2D or 3D grid arrangements, as diagonal edges will be pruned.     - 'keep_axis_aligned': Keep only edges that align with tile axes. This is useful for regular grid         arrangements and to explicitely prune diagonals, e.g. when other methods fail. post_registration_do_quality_filter : bool, optional post_registration_quality_threshold : float, optional     Threshold used to filter edges by quality after registration,     by default None (no filtering) plot_summary : bool, optional     If True, plot a graph showing registered stack boundaries and     performed pairwise registrations including correlations, by default False pairs : list of tuples, optional     If set, initialises the view adjacency graph using the indicates     pairs of view/tile indices, by default None</p>"},{"location":"api/registration/#multiview_stitcher.registration.register--returns","title":"Returns","text":"<p>list of xr.DataArray     Parameters mapping each view into a new extrinsic coordinate system</p> Source code in <code>src/multiview_stitcher/registration.py</code> <pre><code>def register(\n    msims: list[MultiscaleSpatialImage],\n    transform_key,\n    reg_channel_index=None,\n    reg_channel=None,\n    new_transform_key=None,\n    registration_binning=None,\n    overlap_tolerance=0.0,\n    pairwise_reg_func=phase_correlation_registration,\n    pairwise_reg_func_kwargs=None,\n    groupwise_resolution_method=\"global_optimization\",\n    groupwise_resolution_kwargs=None,\n    pre_registration_pruning_method=\"alternating_pattern\",\n    post_registration_do_quality_filter=False,\n    post_registration_quality_threshold=0.2,\n    plot_summary=False,\n    pairs=None,\n    scheduler=None,\n):\n    \"\"\"\n\n    Register a list of views to a common extrinsic coordinate system.\n\n    This function is the main entry point for registration.\n\n    1) Build a graph of pairwise overlaps between views\n    2) Determine registration pairs from this graph\n    3) Register each pair of views.\n       Need to add option to pass registration functions here.\n    4) Determine the parameters mapping each view into the new extrinsic\n       coordinate system.\n       Currently done by determining a reference view and concatenating for reach\n       view the pairwise transforms along the shortest paths towards the ref view.\n\n    Parameters\n    ----------\n    msims : list of MultiscaleSpatialImage\n        Input views\n    reg_channel_index : int, optional\n        Index of channel to be used for registration, by default None\n    reg_channel : str, optional\n        Name of channel to be used for registration, by default None\n        Overrides reg_channel_index\n    transform_key : str, optional\n        Extrinsic coordinate system to use as a starting point\n        for the registration, by default None\n    new_transform_key : str, optional\n        If set, the registration result will be registered as a new extrinsic\n        coordinate system in the input views (with the given name), by default None\n    registration_binning : dict, optional\n        Binning applied to each dimensionn during registration, by default None\n    overlap_tolerance : float, optional\n        Extend overlap regions considered for pairwise registration.\n        - if 0, the overlap region is the intersection of the bounding boxes.\n        - if &gt; 0, the overlap region is the intersection of the bounding boxes\n            extended by this value in all spatial dimensions.\n        - if None, the full images are used for registration\n    pairwise_reg_func : func, optional\n        Function used for registration.\n    pairwise_reg_func_kwargs : dict, optional\n        Additional keyword arguments passed to the registration function\n    groupwise_resolution_method : str, optional\n        Method used to determine the final transform parameters\n        from pairwise registrations:\n        - 'global_optimization': global optimization considering all pairwise transforms\n        - 'shortest_paths': concatenation of pairwise transforms along shortest paths\n    groupwise_resolution_kwargs : dict, optional\n        Additional keyword arguments passed to the groupwise optimization function\n    pre_registration_pruning_method : str, optional\n        Method used to eliminate registration edges (e.g. diagonals) from the view adjacency\n        graph before registration. Available methods:\n        - None: No pruning, useful when no regular arrangement is present.\n        - 'alternating_pattern': Prune to edges between squares of differering\n            colors in checkerboard pattern. Useful for regular 2D tile arrangements (of both 2D or 3D data).\n        - 'shortest_paths_overlap_weighted': Prune to shortest paths in overlap graph\n            (weighted by overlap). Useful to minimize the number of pairwise registrations.\n        - 'otsu_threshold_on_overlap': Prune to edges with overlap above Otsu threshold.\n            This is useful for regular 2D or 3D grid arrangements, as diagonal edges will be pruned.\n        - 'keep_axis_aligned': Keep only edges that align with tile axes. This is useful for regular grid\n            arrangements and to explicitely prune diagonals, e.g. when other methods fail.\n    post_registration_do_quality_filter : bool, optional\n    post_registration_quality_threshold : float, optional\n        Threshold used to filter edges by quality after registration,\n        by default None (no filtering)\n    plot_summary : bool, optional\n        If True, plot a graph showing registered stack boundaries and\n        performed pairwise registrations including correlations, by default False\n    pairs : list of tuples, optional\n        If set, initialises the view adjacency graph using the indicates\n        pairs of view/tile indices, by default None\n\n    Returns\n    -------\n    list of xr.DataArray\n        Parameters mapping each view into a new extrinsic coordinate system\n    \"\"\"\n\n    if pairwise_reg_func_kwargs is None:\n        pairwise_reg_func_kwargs = {}\n\n    if groupwise_resolution_kwargs is None:\n        groupwise_resolution_kwargs = {}\n\n    sims = [msi_utils.get_sim_from_msim(msim) for msim in msims]\n\n    if \"c\" in msi_utils.get_dims(msims[0]):\n        if reg_channel is None:\n            if reg_channel_index is None:\n                for msim in msims:\n                    if \"c\" in msi_utils.get_dims(msim):\n                        raise (\n                            Exception(\"Please choose a registration channel.\")\n                        )\n            else:\n                reg_channel = sims[0].coords[\"c\"][reg_channel_index]\n\n        msims_reg = [\n            msi_utils.multiscale_sel_coords(msim, {\"c\": reg_channel})\n            if \"c\" in msi_utils.get_dims(msim)\n            else msim\n            for imsim, msim in enumerate(msims)\n        ]\n    else:\n        msims_reg = msims\n\n    g = mv_graph.build_view_adjacency_graph_from_msims(\n        msims_reg,\n        transform_key=transform_key,\n        pairs=pairs,\n    )\n\n    if pre_registration_pruning_method is not None:\n        g_reg = prune_view_adjacency_graph(\n            g,\n            method=pre_registration_pruning_method,\n        )\n    else:\n        g_reg = g\n\n    g_reg_computed = compute_pairwise_registrations(\n        msims_reg,\n        g_reg,\n        transform_key=transform_key,\n        registration_binning=registration_binning,\n        overlap_tolerance=overlap_tolerance,\n        pairwise_reg_func=pairwise_reg_func,\n        pairwise_reg_func_kwargs=pairwise_reg_func_kwargs,\n        scheduler=scheduler,\n    )\n\n    if post_registration_do_quality_filter:\n        # filter edges by quality\n        g_reg_computed = mv_graph.filter_edges(\n            g_reg_computed,\n            threshold=post_registration_quality_threshold,\n            weight_key=\"quality\",\n        )\n\n    params, groupwise_opt_info = groupwise_resolution(\n        g_reg_computed,\n        method=groupwise_resolution_method,\n        **groupwise_resolution_kwargs,\n    )\n\n    params = [params[iview] for iview in sorted(g_reg_computed.nodes())]\n\n    if new_transform_key is not None:\n        for imsim, msim in enumerate(msims):\n            msi_utils.set_affine_transform(\n                msim,\n                params[imsim],\n                transform_key=new_transform_key,\n                base_transform_key=transform_key,\n            )\n\n        if plot_summary:\n            edges = list(g_reg_computed.edges())\n            _fig, _ax = vis_utils.plot_positions(\n                msims,\n                transform_key=new_transform_key,\n                edges=edges,\n                edge_color_vals=np.array(\n                    [\n                        g_reg_computed.get_edge_data(*e)[\"quality\"].mean()\n                        for e in edges\n                    ]\n                ),\n                edge_label=\"pairwise view correlation\",\n                display_view_indices=True,\n                use_positional_colors=False,\n            )\n\n    return params\n</code></pre>"}]}